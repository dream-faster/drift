# -*- coding: utf-8 -*-
"""Core Walkthrough - Endogenous and Exogenous.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CVhxOmbHO9PvsdHfGvR91ilJUqEnUuy8

# Fold - Core Walkthrough

![Walkthrough Cover.png](https://lh3.googleusercontent.com/drive-viewer/AAOQEOT4w3Cu4i_TPzDV4WAHd3DkRuz7-4rPsre2jX05y_oanG19aCCMmi_oglzAKdRGZ-qYYTUdSoJJxE7KSc_wNkCT3GAsCA=s1600)

**Welcome ðŸ‘‹**

In this notebook we'll demonstrate `fold`'s powerful interface for creating, training, and cross-validating (or backtesting, if you prefer) simple and *composite* models/pipelines.

We will use the dataset from an [Energy residual load forcasting challenge](https://www.kaggle.com/competitions/energy-forecasting-data-challenge) hosted on Kaggle.

---

**By the end you will know how to:**
- Create a simple and ensemble model (composite model)
- Train multiple models / pipelines over time
- Analyze the model's simulated past performance

---

Let's start by installing:
- [`fold`](https://github.com/dream-faster/fold)
- [`fold-models`](https://github.com/dream-faster/fold-models): optional, this will be required later for third party models. Wraps eg. `XGBoost` or `StatsForecast` models to be used with `fold`.
- [`krisi`](https://github.com/dream-faster/krisi), optional. Dream Faster's Time-Series evaluation library to quickly get results.

### Installing libraries
"""

import os

os.system(
    "pip install --quiet https://github.com/dream-faster/fold/archive/main.zip https://github.com/dream-faster/fold-models/archive/main.zip git+https://github.com/dream-faster/krisi.git@main matplotlib seaborn xgboost plotly prophet statsforecast statsmodels ray"
)


"""# Data Loading and Exploration

Let's load in the data and do minimal exploration of the structure of the data. 

`fold` has a useful utility function that loads example data from our [`datasets`](https://github.com/dream-faster/datasets) GitHub repo. 

*   We are forecasting `residual_load`â€¡. 
*   We will shorten the dataset to `4000` rows so we have a speedier demonstration.


---

â€¡ *The difference between the `load` in the network and the `P` that the industrial complex is producing.*
"""


from fold.utils.dataset import get_preprocessed_dataset

X, y = get_preprocessed_dataset(
    "energy/industrial_pv_load",
    target_col="residual_load",
    resample="H",
    deduplication_strategy="first",
    shorten=4000,
)
no_of_observation_per_day = 24
no_of_observation_per_week = no_of_observation_per_day * 7

y.plot(figsize=(20, 5), grid=True)

"""The data format may be very familiar - it looks like the standard scikit-learn data.

`X` represents exogenous variables in `fold`, where a single row corresponds to a single target value. That means we currently only support univariate time-series (with exogenous variables), but soon we're extending that.

It's important that the data should be sorted and its integrity (no missing values, no duplicate indicies) should be checked before passing data to `fold`.
"""

X.head()

"""(We'll ignore the exogenous variables until a bit later)"""

y.head()

"""You can see that `y` (our target) contains the next value of `X`'s "residual_load" column.

# Time Series Cross Validation with a univariate forecaster
---

### 1. Model Building

`fold` has three core type of building blocks which you can build arbitrary sophisticated pipelines from:
- **Transformations** (classes that change, augment the data. eg: `AddHolidayFeatures` adds a column feature of holidays/weekends to your exogenous variables)
- **Models** (eg.: Sklearn, Baseline Models, third-party adapters from [`fold-models`](https://github.com/dream-faster/fold-models), like Statsmodels)
- **Composites** (eg.: `Ensemble` - takes the mean of the output of arbitrary number of 'parallel' models or pipelines)

Let's use Facebook's popular [`Prophet`](https://facebook.github.io/prophet/) library, and create in instance.

If [`fold-models`](https://github.com/dream-faster/fold-models) is installed, `fold` can take this instance without any additional wrapper class.
"""

from prophet import Prophet

prophet = Prophet()

"""### 2. Creating a Splitter

A splitter allows us to do Time Series Cross-Validation with various strategies.

`fold` supports three types of `Splitters`:
![Splitter](https://lh3.googleusercontent.com/drive-viewer/AAOQEOSAR7ICe29LSRo8umKyNIC-2c32LLdo46vSB30bwiJbYGMwFtc22rEtyWy62Eu7A0yDLimaEXOjgXx-4_PS92sqMgb0ww=s1600)
"""

from fold.splitters import ExpandingWindowSplitter

splitter = ExpandingWindowSplitter(
    initial_train_window=no_of_observation_per_week * 6, step=no_of_observation_per_week
)

"""Here, `initial_train_window` defines the first window size, `step` is the size of the window between folds.

We're gonna be using the first 6 weeks as our initial window, and re-train (or update, in another training mode) it every week after. We'll have 18 models, each predicting the next week's target variable.

You can also use percentages to define both, for example, `0.1` would be equivalent to `10%` of the availabel data.

### 3. Training a (univariate) Model

We could use [ray](https://www.ray.io/) to parallelize the training of multiple folds, halving the time it takes for every CPU core we have available (or deploying it to a cluster, if needed).

We pass in `None` as `X`, to indicate that we want to train a univariate model, without any exogenous variables.
"""

import ray

from fold import Backend, train_evaluate

ray.init(ignore_reinit_error=True)

scorecard, predictions, trained_pipeline = train_evaluate(
    prophet,
    None,
    y,
    splitter,
    backend=Backend.ray,
    krisi_args={"model_name": "prophet"},
)

"""### 4. Evaluating the results"""

scorecard.print("minimal")

from krisi.report import plot_y_predictions

plot_y_predictions(
    y[predictions.index], predictions, mode="overlap", y_name="residual_load"
)

"""Finally, let's save the scorecard into a list, so we can compare the results later."""

results = [(scorecard, predictions)]

"""# Using an Ensemble (Composite) model
---

Here we will build an `Ensemble` model that leverages the output of multiple models. 

![Ensembling Models.png](https://lh3.googleusercontent.com/drive-viewer/AAOQEORyLi4ZPadHho7_C_IMdxDHxoZOt7T-y-7vMmTJ4BTubYk_4xu6hntPuK3nY1HmS4GC3DDQCKWgyqKQijheEhclhz_qYw=s1600)

### 1. Model Building with `fold-models`

We are going to define three different pipelines, each leveraging a different model and different features.

We can leverage the most popular modelling libraries, like StatsForecast, Sktime, XGBoost, etc. (the list can be found [here](https://github.com/dream-faster/fold-models)).

Let's train a [MSTL](https://arxiv.org/abs/2107.13462) model that's implemented in [StatsForecast](https://nixtla.github.io/statsforecast/models.html), that can capture multiple seasonalities, with the `WrapStatsForecast` class from `fold-models`. This is not strictly necessary, though, as the automatic wrapping also works for StatsForecast instaces as well.
"""

from fold_models import WrapStatsForecast
from statsforecast.models import MSTL

mstl = WrapStatsForecast.from_model(MSTL([24, 168]))

"""### 2. Ensembling with `fold`

Finally, let's `ensemble` the two pipelines.
"""

from fold.composites import Ensemble

univariate_ensemble = Ensemble([prophet, mstl])

"""### 3. Training all pipelines seperately and within an `ensemble`

We'll use the same `ExpandingWindowSplitter` we have defined above, to make performance comparable.
"""

from fold import train_evaluate

for name, pipeline in [("mstl", mstl), ("univariate_ensemble", univariate_ensemble)]:
    scorecard, predictions, pipeline_trained = train_evaluate(
        pipeline, None, y, splitter, krisi_args={"model_name": name}
    )
    results.append((scorecard, predictions))

from krisi import compare

compare([scorecard for scorecard, predictions in results])

"""We see that our Ensemble model has beaten all individual models' performance - which is very usual in the time series context.

# Using a single-step ahead forecaster (a baseline)

So far we've used models that were costly to update (or re-train) every day, therefore we were limited to training once for every week, then predicting the next week's target.

What if we could use a lightweight, "online" model, that can be updated on every timestamp?

And.. what if we just repeat the last value?

That'd be the `Naive` model you can load from `fold_models`.
"""

from fold_models import Naive

from fold import train_evaluate

scorecard, predictions, trained_pipeline = train_evaluate(
    Naive(), None, y, splitter, krisi_args={"model_name": "naive"}
)
results.append((scorecard, predictions))
scorecard.print("minimal")

"""**We call this [Continous Validation](https://dream-faster.github.io/fold/concepts/continuous-validation/).**

It looks like having access to last value really makes a difference: the baseline model beats all long-term forecasting models by a large margin.

**It's extremely important to define our forecasting task well**:
1. We need to think about what time horizon can and should forecast
2. And how frequently can we update our models.

Long-horizon (in this case, a week ahead) forecasts can be very unreliable, on the other hand, frequent, short-term forecasts are where Machine Learning shines (as we'll see in the next section).

# Using exogenous variables with Tabular Models
---

So far we have been training univariate models, and ignored all the additional, exogenous variables that come with our data.

Let's try whether using this data boost our model's performance!

## Building Models separately


We'll be using scikit-learn's `HistGradientBoostingRegressor`, their competing implementation of Gradient Boosted Trees. You don't need to wrap `scikit-learn` models or transformations when using it in `fold`, just pass it in directly to any pipeline.
"""

from sklearn.ensemble import HistGradientBoostingRegressor

tree_model = HistGradientBoostingRegressor(max_depth=10)

"""


Let's add both holiday and date-time features to our previous ensemble pipeline.

The data was gathered in the Region of Hessen, Germany -- so we pass in `DE` (we can pass in multiple regions). This transformation adds another column for holidays to our `exogenous` (`X`) features.

We're also adding the current hour, and day of week as integers to our exogenous features. This is one of the ways for our tabular model to capture seasonality.
"""

from fold.transformations import AddDateTimeFeatures, AddHolidayFeatures

datetime = AddDateTimeFeatures(["hour", "day_of_week", "day_of_year"])
holidays = AddHolidayFeatures(["DE"])

"""
Let's add a couple of lagged, exogenous values for our model. `AddLagsX` receives a tuple of column name and integer or list of lags, for each of which it will create a column in `X`.

We can easily create transformations of existing features on a rolling window basis with `AddWindowFeatures` as well, in this case, the last day's average value for all of our exogenous features.

We can "tie in" two separate pipelines with `Concat`, which concatenates all columns from all sources."""

from fold.composites import Concat
from fold.transformations import AddLagsX, AddWindowFeatures

tree = [
    Concat(
        [
            AddLagsX(("all", range(1, 3))),
            AddWindowFeatures([("all", 24, "mean")]),
        ]
    ),
    datetime,
    holidays,
    tree_model,
]

"""Let's see how this performs!

We can also use fold's `train`, `backtest` to decouple these functionalities.
"""

from krisi import score

from fold import backtest, train

trained_pipeline = train(tree, X, y, splitter)
predictions = backtest(trained_pipeline, X, y, splitter)
scorecard = score(
    y[predictions.index], predictions.squeeze(), model_name="tabular_tree"
)

results.append((scorecard, predictions))
scorecard.print("minimal")

"""## Creating an Ensemble of Tabular models

First let's creat two more models:
* an Sklearn LinearRegressor
* and an XGBoostRegressor instance

We are also going to use the HistGradientBoostingRegressor pipeline that we defined prior.

"""

from sklearn.linear_model import LinearRegression

lregression = [AddLagsX(("all", range(1, 3))), datetime, LinearRegression()]

from fold_models.xgboost import WrapXGB
from xgboost import XGBRegressor

xgboost = [AddLagsX(("all", range(1, 3))), datetime, WrapXGB.from_model(XGBRegressor())]

tabular_ensemble = Ensemble([lregression, xgboost, tree])

scorecard, predictions, pipeline_trained = train_evaluate(
    tabular_ensemble, X, y, splitter, krisi_args={"model_name": "tabular_ensemble"}
)
results.append((scorecard, predictions))

"""# Comparing & Vizualising the results
---
"""

compare([scorecard for scorecard, _ in results])

"""In this simplistic, unfair comparison, it looks like the tabular models (and the Naive baseline) that have access to the previous value (and the exogenous variables) outperform the univariate models that are only re-trained every week. 

We can't really draw general conclusions from this work, though. 

Unlike NLP and Computer vision, Time Series data is very heterogeneous, and a Machine Learning approach that works well for one series may be an inferior choice for your specific usecase.

---

But now we have an easy way to compare the different pipelines, with unprecedented speed, by using a unified interface, with [fold](https://github.com/dream-faster/fold). 



"""

all_predictions = [
    predictions.squeeze().rename(scorecard.metadata.model_name)
    for scorecard, predictions in results
]

plot_y_predictions(
    y[predictions.index], all_predictions, y_name="residual_load", mode="seperate"
)

"""Want to know more?
Visit [fold's Examples page](https://dream-faster.github.io/fold/), and access all the necessary snippets you need for you to build a Time Series ML pipeline!
"""
